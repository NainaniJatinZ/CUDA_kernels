{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN8BRKFOj7PlzrwkQODYY5w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NainaniJatinZ/CUDA_kernels/blob/main/profiling_cuda_kernels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuda is asynchronous. If we were to just use python time module, we will just end up measuring the overhead to launch a kernel."
      ],
      "metadata": {
        "id": "0uZDCHFhdNXO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqemXLucdGCs",
        "outputId": "0f233a05-1367-472c-91a0-f88825bb62b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 4., 9.])\n",
            "tensor([1., 4., 9.])\n",
            "tensor([1., 4., 9.])\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor([1., 2., 3.])\n",
        "\n",
        "print(torch.square(a))\n",
        "print(a ** 2)\n",
        "print(a * a)\n",
        "\n",
        "def time_pytorch_function(func, input):\n",
        "    # CUDA IS ASYNC so can't use python time module\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(5):\n",
        "        func(input)\n",
        "\n",
        "    start.record()\n",
        "    func(input)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return start.elapsed_time(end)\n",
        "\n",
        "b = torch.randn(10000, 10000).cuda()\n",
        "\n",
        "def square_2(a):\n",
        "    return a * a\n",
        "\n",
        "def square_3(a):\n",
        "    return a ** 2\n",
        "\n",
        "time_pytorch_function(torch.square, b)\n",
        "time_pytorch_function(square_2, b)\n",
        "time_pytorch_function(square_3, b)\n",
        "\n",
        "print(\"=============\")\n",
        "print(\"Profiling torch.square\")\n",
        "print(\"=============\")\n",
        "# Now profile each function using pytorch profiler\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "    torch.square(b)\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
        "\n",
        "print(\"=============\")\n",
        "print(\"Profiling a * a\")\n",
        "print(\"=============\")\n",
        "\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "    square_2(b)\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
        "\n",
        "print(\"=============\")\n",
        "print(\"Profiling a ** 2\")\n",
        "print(\"=============\")\n",
        "\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "    square_3(b)\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl1NkHrxgzv8",
        "outputId": "6c080a79-d556-464f-be5a-e1738cdf8ba3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 4., 9.])\n",
            "tensor([1., 4., 9.])\n",
            "tensor([1., 4., 9.])\n",
            "=============\n",
            "Profiling torch.square\n",
            "=============\n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "             aten::square         0.90%      30.000us         3.25%     108.000us     108.000us      32.000us         0.95%       3.374ms       3.374ms             1  \n",
            "                aten::pow         1.65%      55.000us         2.22%      74.000us      74.000us       3.333ms        98.78%       3.342ms       3.342ms             1  \n",
            "        aten::result_type         0.03%       1.000us         0.03%       1.000us       1.000us       5.000us         0.15%       5.000us       5.000us             1  \n",
            "                 aten::to         0.00%       0.000us         0.00%       0.000us       0.000us       4.000us         0.12%       4.000us       4.000us             1  \n",
            "          cudaEventRecord         0.57%      19.000us         0.57%      19.000us       2.375us       0.000us         0.00%       0.000us       0.000us             8  \n",
            "         cudaLaunchKernel         0.39%      13.000us         0.39%      13.000us      13.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "    cudaDeviceSynchronize        96.45%       3.208ms        96.45%       3.208ms       3.208ms       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 3.326ms\n",
            "Self CUDA time total: 3.374ms\n",
            "\n",
            "=============\n",
            "Profiling a * a\n",
            "=============\n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                aten::mul         1.31%      43.000us         1.77%      58.000us      58.000us       3.327ms       100.00%       3.327ms       3.327ms             1  \n",
            "          cudaEventRecord         0.37%      12.000us         0.37%      12.000us       6.000us       0.000us         0.00%       0.000us       0.000us             2  \n",
            "         cudaLaunchKernel         0.46%      15.000us         0.46%      15.000us      15.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "    cudaDeviceSynchronize        97.86%       3.204ms        97.86%       3.204ms       3.204ms       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 3.274ms\n",
            "Self CUDA time total: 3.327ms\n",
            "\n",
            "=============\n",
            "Profiling a ** 2\n",
            "=============\n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                aten::pow         1.85%      61.000us         2.54%      84.000us      84.000us       3.339ms        99.58%       3.353ms       3.353ms             1  \n",
            "        aten::result_type         0.03%       1.000us         0.03%       1.000us       1.000us       8.000us         0.24%       8.000us       8.000us             1  \n",
            "                 aten::to         0.00%       0.000us         0.00%       0.000us       0.000us       6.000us         0.18%       6.000us       6.000us             1  \n",
            "          cudaEventRecord         0.61%      20.000us         0.61%      20.000us       3.333us       0.000us         0.00%       0.000us       0.000us             6  \n",
            "         cudaLaunchKernel         0.42%      14.000us         0.42%      14.000us      14.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "    cudaDeviceSynchronize        97.10%       3.209ms        97.10%       3.209ms       3.209ms       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 3.305ms\n",
            "Self CUDA time total: 3.353ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REMEMBER: to run appropriate warm up before running profiler to get accurate comparisons.\n",
        "\n",
        " Looking at profiler results:\n",
        "\n",
        " 1. Not really using square, but relying on pow and setting its value to 2\n",
        " 2. For a*a, its using mul. Slightly faster than using pow\n",
        " 3. when using python ** operator, it used pow directly"
      ],
      "metadata": {
        "id": "LqNeFX2ghFCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install ninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juBFbHgbzuyD",
        "outputId": "0001a563-7d57-4d93-e4a5-47d8e93c3680"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inlining cpp\n"
      ],
      "metadata": {
        "id": "AP2LyTrS0zUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hello world example"
      ],
      "metadata": {
        "id": "4IkPXe1B1gwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the next cell and check for the my_module_build folder, main.cpp and build.ninja"
      ],
      "metadata": {
        "id": "I5DHmRp8043K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "\n",
        "cpp_source = \"\"\"\n",
        "std::string hello_world() {\n",
        "  return \"Hello World!\";\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Optionally, specify a new build directory\n",
        "build_directory = '/content/my_module_build'\n",
        "os.makedirs(build_directory, exist_ok=True)\n",
        "\n",
        "# Load the module, forcing a rebuild\n",
        "my_module = load_inline(\n",
        "    name='my_module',\n",
        "    cpp_sources=[cpp_source],\n",
        "    functions=['hello_world'],\n",
        "    verbose=True,\n",
        "    build_directory=build_directory\n",
        ")\n",
        "\n",
        "print(my_module.hello_world())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIx-IRZCx29L",
        "outputId": "3906dc27-3fcc-4202-ab9b-5c9faaf73570"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The input conditions for extension module my_module have changed. Bumping to version 1 and re-building as my_module_v1...\n",
            "Emitting ninja build file /content/my_module_build/build.ninja...\n",
            "Building extension module my_module_v1...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading extension module my_module_v1...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Better example"
      ],
      "metadata": {
        "id": "u9354akU1jRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at this test for inspiration\n",
        "# https://github.com/pytorch/pytorch/blob/main/test/test_cpp_extensions_jit.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "\n",
        "# Define the CUDA kernel and C++ wrapper\n",
        "cuda_source = '''\n",
        "__global__ void square_matrix_kernel(const float* matrix, float* result, int width, int height) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < height && col < width) {\n",
        "        int idx = row * width + col;\n",
        "        result[idx] = matrix[idx] * matrix[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "torch::Tensor square_matrix(torch::Tensor matrix) {\n",
        "    const auto height = matrix.size(0);\n",
        "    const auto width = matrix.size(1);\n",
        "\n",
        "    auto result = torch::empty_like(matrix);\n",
        "\n",
        "    dim3 threads_per_block(16, 16);\n",
        "    dim3 number_of_blocks((width + threads_per_block.x - 1) / threads_per_block.x,\n",
        "                          (height + threads_per_block.y - 1) / threads_per_block.y);\n",
        "\n",
        "    square_matrix_kernel<<<number_of_blocks, threads_per_block>>>(\n",
        "        matrix.data_ptr<float>(), result.data_ptr<float>(), width, height);\n",
        "\n",
        "    return result;\n",
        "    }\n",
        "'''\n",
        "\n",
        "cpp_source = \"torch::Tensor square_matrix(torch::Tensor matrix);\"\n",
        "\n",
        "# Optionally, specify a new build directory\n",
        "build_directory = '/content/square_mat_build'\n",
        "os.makedirs(build_directory, exist_ok=True)\n",
        "\n",
        "# Load the CUDA kernel as a PyTorch extension\n",
        "square_matrix_extension = load_inline(\n",
        "    name='square_matrix_extension',\n",
        "    cpp_sources=cpp_source,\n",
        "    cuda_sources=cuda_source,\n",
        "    functions=['square_matrix'],\n",
        "    with_cuda=True,\n",
        "    extra_cuda_cflags=[\"-O2\"],\n",
        "    build_directory=build_directory,\n",
        "    # extra_cuda_cflags=['--expt-relaxed-constexpr']\n",
        ")\n",
        "\n",
        "a = torch.tensor([[1., 2., 3.], [4., 5., 6.]], device='cuda')\n",
        "print(square_matrix_extension.square_matrix(a))\n",
        "\n",
        "# (cudamode) ubuntu@ip-172-31-9-217:~/cudamode/cudamodelecture1$ python load_inline.py\n",
        "# tensor([[ 1.,  4.,  9.],\n",
        "#         [16., 25., 36.]], device='cuda:0')\n",
        "\n",
        "\n",
        "## No great interaction with ncu\n",
        "\n",
        "# (cudamode) ubuntu@ip-172-31-9-217:~/cudamode/cudamodelecture1$ ncu python load_inline.py\n",
        "# ==PROF== Connected to process 55916 (/opt/conda/envs/cudamode/bin/python3.10)\n",
        "# /opt/conda/envs/cudamode/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 36: API call is not supported in the installed CUDA driver (Triggered internally at /opt/conda/conda-bld/pytorch_1702400410390/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
        "#   return torch._C._cuda_getDeviceCount() > 0\n",
        "# No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
        "# Traceback (most recent call last):\n",
        "#   File \"/home/ubuntu/cudamode/cudamodelecture1/load_inline.py\", line 7, in <module>\n",
        "#     a = torch.tensor([[1., 2., 3.], [4., 5., 6.]], device='cuda')\n",
        "#   File \"/opt/conda/envs/cudamode/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 298, in _lazy_init\n",
        "#     torch._C._cuda_init()\n",
        "# RuntimeError: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 36: API call is not supported in the installed CUDA driver\n",
        "# ==PROF== Disconnected from process 55916\n",
        "# ==ERROR== The application returned an error code (1).\n",
        "# ==WARNING== No kernels were profiled.\n",
        "# ==WARNING== Profiling kernels launched by child processes requires the --target-processes all option."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcwh3DLo1ka1",
        "outputId": "9dc2b1cc-89ea-4eda-d696-1307a27f46cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.,  4.,  9.],\n",
            "        [16., 25., 36.]], device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}